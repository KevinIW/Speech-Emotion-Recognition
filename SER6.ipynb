{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd33ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "import traceback\n",
    "from einops import rearrange\n",
    "from transformers import Wav2Vec2Model, WavLMModel, HubertModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================\n",
    "# 1. SETUP AND CONFIGURATION\n",
    "# =============================================\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('checkpoints', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd6719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 2. DATA LOADING AND EXPLORATION\n",
    "# =============================================\n",
    "\n",
    "# Load data\n",
    "\n",
    "emotion_mapping = {\n",
    "        \"marah\": 0, \"jijik\": 1, \"takut\": 2,\n",
    "        \"bahagia\": 3, \"netral\": 4, \"sedih\": 5\n",
    "    }\n",
    "\n",
    "def load_data():\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    \n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Split into train/val\n",
    "    train_data, val_data = train_test_split(\n",
    "        train_df, test_size=0.15, random_state=42, stratify=train_df['label']\n",
    "    )\n",
    "    \n",
    "    print(f\"Training data: {len(train_data)} samples\")\n",
    "    print(f\"Validation data: {len(val_data)} samples\")\n",
    "    print(f\"Test data: {len(test_df)} samples\")\n",
    "    \n",
    "    return train_df, test_df, train_data, val_data, emotion_mapping\n",
    "\n",
    "# Analyze data distributions\n",
    "def analyze_data_distribution(train_data, val_data, test_df):\n",
    "    # Count the emotion distributions\n",
    "    train_dist = train_data['label'].value_counts().sort_index()\n",
    "    val_dist = val_data['label'].value_counts().sort_index()\n",
    "    \n",
    "    # Check if test data has labels\n",
    "    if 'label' in test_df.columns:\n",
    "        test_dist = test_df['label'].value_counts().sort_index()\n",
    "        has_test_labels = True\n",
    "    else:\n",
    "        has_test_labels = False\n",
    "    \n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Training distribution\n",
    "    ax1 = plt.subplot(1, 3, 1)\n",
    "    sns.barplot(x=train_dist.index, y=train_dist.values, palette='viridis')\n",
    "    plt.title('Training Emotion Distribution')\n",
    "    plt.xlabel('Emotion')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add count labels on the bars\n",
    "    for i, count in enumerate(train_dist.values):\n",
    "        plt.text(i, count + 5, str(count), ha='center', fontweight='bold')\n",
    "\n",
    "    # Validation distribution\n",
    "    ax2 = plt.subplot(1, 3, 2)\n",
    "    sns.barplot(x=val_dist.index, y=val_dist.values, palette='viridis')\n",
    "    plt.title('Validation Emotion Distribution')\n",
    "    plt.xlabel('Emotion')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, count in enumerate(val_dist.values):\n",
    "        plt.text(i, count + 1, str(count), ha='center', fontweight='bold')\n",
    "    \n",
    "    # Test distribution if available\n",
    "    if has_test_labels:\n",
    "        ax3 = plt.subplot(1, 3, 3)\n",
    "        sns.barplot(x=test_dist.index, y=test_dist.values, palette='viridis')\n",
    "        plt.title('Test Emotion Distribution')\n",
    "        plt.xlabel('Emotion')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        for i, count in enumerate(test_dist.values):\n",
    "            plt.text(i, count + 1, str(count), ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"Emotion Distribution:\")\n",
    "    print(\"\\nTraining Dataset:\")\n",
    "    train_percent = (train_dist / train_dist.sum() * 100).round(2)\n",
    "    for emotion, count in zip(train_dist.index, train_dist.values):\n",
    "        print(f\"{emotion}: {count} samples ({train_percent[emotion]}%)\")\n",
    "    \n",
    "    print(\"\\nValidation Dataset:\")\n",
    "    val_percent = (val_dist / val_dist.sum() * 100).round(2)\n",
    "    for emotion, count in zip(val_dist.index, val_dist.values):\n",
    "        print(f\"{emotion}: {count} samples ({val_percent[emotion]}%)\")\n",
    "    \n",
    "    # Create pie charts\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.pie(train_dist.values, labels=train_dist.index, autopct='%1.1f%%', startangle=90, shadow=True)\n",
    "    plt.title('Training Data Emotion Distribution')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.pie(val_dist.values, labels=val_dist.index, autopct='%1.1f%%', startangle=90, shadow=True)\n",
    "    plt.title('Validation Data Emotion Distribution')\n",
    "    \n",
    "    if has_test_labels:\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.pie(test_dist.values, labels=test_dist.index, autopct='%1.1f%%', startangle=90, shadow=True)\n",
    "        plt.title('Test Data Emotion Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 3. DATASET CLASSES\n",
    "# =============================================\n",
    "\n",
    "\n",
    "class RawAudioEmotionDataset(Dataset):\n",
    "    def __init__(self, df, base_path, use_cache=True):\n",
    "        self.df = df\n",
    "        self.base_path = base_path\n",
    "        self.class2idx = {\"marah\":0, \"jijik\":1, \"takut\":2, \"bahagia\":3, \"netral\":4, \"sedih\":5}\n",
    "        self.use_cache = use_cache\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Audio preprocessing settings\n",
    "        self.target_sr = 16000  # Wav2Vec2 expects 16kHz audio\n",
    "        self.max_length = 160000  # 10 seconds max at 16kHz\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get audio path and label\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_id = row['id']\n",
    "        audio_path = f\"{self.base_path}/{audio_id}\"\n",
    "        \n",
    "        # Use cache if available\n",
    "        if self.use_cache and audio_path in self.cache:\n",
    "            return self.cache[audio_path]\n",
    "            \n",
    "        try:\n",
    "            # Load audio\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            \n",
    "            # Convert stereo to mono\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "            \n",
    "            # Resample to target sample rate\n",
    "            if sample_rate != self.target_sr:\n",
    "                waveform = torchaudio.functional.resample(waveform, sample_rate, self.target_sr)\n",
    "            \n",
    "            # Normalize waveform\n",
    "            waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-8)\n",
    "            \n",
    "            # Pad or truncate to max_length\n",
    "            if waveform.shape[1] > self.max_length:\n",
    "                waveform = waveform[:, :self.max_length]\n",
    "            else:\n",
    "                padding_length = self.max_length - waveform.shape[1]\n",
    "                waveform = F.pad(waveform, (0, padding_length), \"constant\", 0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {e}\")\n",
    "            waveform = torch.zeros(1, self.max_length)\n",
    "        \n",
    "        # Get label (or None for test data)\n",
    "        if 'label' in row:\n",
    "            label = self.class2idx[row['label']]\n",
    "            result = (waveform, torch.tensor(label))\n",
    "        else:\n",
    "            result = (waveform, audio_id)\n",
    "            \n",
    "        # Store in cache\n",
    "        if self.use_cache:\n",
    "            self.cache[audio_path] = result\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class FastRawAudioEmotionDataset(RawAudioEmotionDataset):\n",
    "    def __init__(self, df, base_path, use_cache=True):\n",
    "        super().__init__(df, base_path, use_cache)\n",
    "        self.max_length = 48000  # 3 seconds at 16kHz instead of 10s\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get audio path and label\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_id = row['id']\n",
    "        audio_path = f\"{self.base_path}/{audio_id}\"\n",
    "        \n",
    "        # Use cache if available\n",
    "        if self.use_cache and audio_path in self.cache:\n",
    "            return self.cache[audio_path]\n",
    "        \n",
    "        try:\n",
    "            # Load audio\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            \n",
    "            # Convert stereo to mono\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "            \n",
    "            # Resample if needed\n",
    "            if sample_rate != self.target_sr:\n",
    "                waveform = torchaudio.functional.resample(waveform, sample_rate, self.target_sr)\n",
    "            \n",
    "            # Normalize\n",
    "            waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-8)\n",
    "            \n",
    "            # Take center 3 seconds for consistency\n",
    "            if waveform.shape[1] > self.max_length:\n",
    "                start = (waveform.shape[1] - self.max_length) // 2\n",
    "                waveform = waveform[:, start:start + self.max_length]\n",
    "            else:\n",
    "                padding_length = self.max_length - waveform.shape[1]\n",
    "                waveform = F.pad(waveform, (0, padding_length), \"constant\", 0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {e}\")\n",
    "            waveform = torch.zeros(1, self.max_length)\n",
    "        \n",
    "        # Get label or ID\n",
    "        if 'label' in row:\n",
    "            label = self.class2idx[row['label']]\n",
    "            result = (waveform, torch.tensor(label))\n",
    "        else:\n",
    "            result = (waveform, audio_id)\n",
    "            \n",
    "        # Store in cache\n",
    "        if self.use_cache:\n",
    "            self.cache[audio_path] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 4. MODEL ARCHITECTURES\n",
    "# =============================================\n",
    "\n",
    "\n",
    "class Wav2Vec2ForSEROptimized(nn.Module):\n",
    "    def __init__(self, num_classes=6, pretrained_model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\", num_layers_to_unfreeze=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        from transformers import AutoModelForAudioClassification\n",
    "        \n",
    "        # Load the specialized SER model\n",
    "        self.model = AutoModelForAudioClassification.from_pretrained(pretrained_model)\n",
    "        \n",
    "        # Get the base Wav2Vec2 model\n",
    "        self.wav2vec = self.model.wav2vec2\n",
    "        \n",
    "        # Freeze all parameters first\n",
    "        for param in self.wav2vec.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Unfreeze the last N transformer layers for fine-tuning\n",
    "        total_layers = len(self.wav2vec.encoder.layers)\n",
    "        print(f\"Model has {total_layers} transformer layers\")\n",
    "        print(f\"Unfreezing the last {num_layers_to_unfreeze} transformer layers\")\n",
    "        \n",
    "        for i in range(1, min(num_layers_to_unfreeze + 1, total_layers + 1)):\n",
    "            layer_idx = total_layers - i\n",
    "            print(f\"Unfreezing layer {layer_idx}\")\n",
    "            for param in self.wav2vec.encoder.layers[layer_idx].parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Feature dimension from the model\n",
    "        hidden_size = self.wav2vec.config.hidden_size\n",
    "        \n",
    "        # Custom attention pooling\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Replace the classifier with our own (adapted to our 6 emotion classes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),  # Increased dropout to 0.4 for better regularization\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),  # Increased dropout to 0.4\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)  # [batch_size, audio_length]\n",
    "        \n",
    "        # Extract features using the pretrained model\n",
    "        outputs = self.wav2vec(x, output_hidden_states=True)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Apply attention pooling\n",
    "        attention_weights = F.softmax(self.attention(hidden_states), dim=1)\n",
    "        attention_output = torch.sum(hidden_states * attention_weights, dim=1)\n",
    "        \n",
    "        # Classification with our custom head\n",
    "        logits = self.classifier(attention_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a436a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 5. TRAINING UTILITIES\n",
    "# =============================================\n",
    "\n",
    "\n",
    "def train_mixed_precision(model, train_loader, val_loader, criterion, optimizer, \n",
    "                         num_epochs=3, device=device, scheduler=None):\n",
    "    best_val_acc = 0.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    # Enable mixed precision if available\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "    \n",
    "    print(\"Starting accelerated training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nStarting Epoch {epoch+1}/{num_epochs}...\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Training epoch {epoch+1}\"):\n",
    "            batch_count += 1\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            if scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Mixed precision backward pass\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Regular forward/backward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "            # Print updates\n",
    "            if batch_count % 50 == 0:\n",
    "                print(f\"  Batch {batch_count}/{len(train_loader)}: loss={loss.item():.4f}\")\n",
    "                \n",
    "        # Calculate training metrics\n",
    "        epoch_train_loss = train_loss / max(1, batch_count)\n",
    "        epoch_train_acc = train_correct / max(1, train_total)\n",
    "        \n",
    "        print(f\"\\nEvaluating epoch {epoch+1}...\")\n",
    "        \n",
    "        # Validation (simplified)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_batch_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                val_batch_count += 1\n",
    "                \n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Mixed precision inference\n",
    "                if scaler:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Track metrics\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        epoch_val_loss = val_loss / max(1, val_batch_count)\n",
    "        epoch_val_acc = val_correct / max(1, val_total)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "        \n",
    "        # Update learning rate\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Summary:\")\n",
    "        print(f\"  Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            torch.save(model.state_dict(), 'checkpoints/best_wav2vec.pth')\n",
    "            print(f\"  ✓ New best model saved with accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    try:\n",
    "        model.load_state_dict(torch.load('checkpoints/best_wav2vec.pth'))\n",
    "        print(\"Loaded best model\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading best model: {e}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def predict(model, test_loader, device, emotion_mapping):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, ids in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_ids.extend(ids)\n",
    "    \n",
    "    # Map predictions back to emotion labels\n",
    "    reverse_mapping = {v: k for k, v in emotion_mapping.items()}\n",
    "    pred_emotions = [reverse_mapping[pred] for pred in all_preds]\n",
    "    \n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': all_ids,\n",
    "        'label': pred_emotions\n",
    "    })\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def plot_training_history(history, title='', save_path=None):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title(f'{title} Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Validation')\n",
    "    plt.title(f'{title} Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Training history plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 6. TRAINING PIPELINES\n",
    "# =============================================\n",
    "\n",
    "\n",
    "def train_wav2vec_optimized(train_data, val_data, test_df, emotion_mapping):\n",
    "    print(\"Creating optimized raw audio datasets...\")\n",
    "    raw_train_dataset = FastRawAudioEmotionDataset(\n",
    "        train_data, \n",
    "        base_path=\"train/\", \n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    raw_val_dataset = FastRawAudioEmotionDataset(\n",
    "        val_data, \n",
    "        base_path=\"train/\", \n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    raw_test_dataset = FastRawAudioEmotionDataset(\n",
    "        test_df, \n",
    "        base_path=\"test/\", \n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    raw_train_loader = DataLoader(\n",
    "        raw_train_dataset, \n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    raw_val_loader = DataLoader(\n",
    "        raw_val_dataset, \n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    raw_test_loader = DataLoader(\n",
    "        raw_test_dataset, \n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model with 3 unfrozen layers\n",
    "    print(\"Initializing optimized Wav2Vec2 model with multiple unfrozen layers...\")\n",
    "    model = Wav2Vec2ForSEROptimized(\n",
    "        num_classes=len(emotion_mapping),\n",
    "        pretrained_model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\",  \n",
    "        num_layers_to_unfreeze=3  \n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model has {total_params:,} parameters ({trainable_params:,} trainable)\")\n",
    "    \n",
    "    # Loss function with label smoothing \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    # Optimizer with slightly lower learning rate for more unfrozen layers\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=2.5e-5,  \n",
    "        weight_decay=0.02\n",
    "    )\n",
    "    \n",
    "    # Better scheduler for longer training\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=7,  \n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Train with mixed precision for 7 epochs\n",
    "    print(f\"\\nStarting enhanced Wav2Vec2 training for 7 epochs...\")\n",
    "    model, history = train_mixed_precision(\n",
    "        model=model,\n",
    "        train_loader=raw_train_loader,\n",
    "        val_loader=raw_val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=7,\n",
    "        device=device,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"\\nGenerating predictions with optimized Wav2Vec2 model...\")\n",
    "    submission_df = predict(model, raw_test_loader, device, emotion_mapping)\n",
    "    submission_df.to_csv('wav2vec_submission.csv', index=False)\n",
    "    print(\"Wav2Vec2 predictions saved to wav2vec_submission.csv\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history, 'Wav2Vec2 Model', 'wav2vec_history.png')\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#infer_emotion function\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = Wav2Vec2ForSEROptimized(num_classes=6)\n",
    "    \n",
    "    # Load the saved weights\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # Move model to appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Loaded Wav2Vec2 model from {model_path}\")\n",
    "    return model, device\n",
    "\n",
    "# Simplify infer_emotion function:\n",
    "def infer_emotion(model, audio_paths, base_dir=\"\"):\n",
    "    reverse_mapping = {0: \"marah\", 1: \"jijik\", 2: \"takut\", 3: \"bahagia\", 4: \"netral\", 5: \"sedih\"}\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Create a DataFrame with just the filenames\n",
    "    file_ids = [os.path.basename(path) for path in audio_paths]\n",
    "    \n",
    "    # Create dataset for inference\n",
    "    example_dataset = FastRawAudioEmotionDataset(\n",
    "        pd.DataFrame({'id': file_ids}),\n",
    "        base_path=base_dir\n",
    "    )\n",
    "    \n",
    "    example_loader = DataLoader(\n",
    "        example_dataset, \n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    probabilities = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in example_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            max_prob, preds = torch.max(probs, 1)\n",
    "            emotion = reverse_mapping[preds.item()]\n",
    "            results.append(emotion)\n",
    "            probabilities.append(max_prob.item())\n",
    "    \n",
    "    return results, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe04784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 7. MAIN EXECUTION\n",
    "# =============================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the SER pipeline with Wav2Vec2 model.\"\"\"\n",
    "    # Load and explore data\n",
    "    print(\"Loading data...\")\n",
    "    train_df, test_df, train_data, val_data, emotion_mapping = load_data()\n",
    "    \n",
    "    # Analyze data distribution\n",
    "    print(\"Analyzing data distribution...\")\n",
    "    analyze_data_distribution(train_data, val_data, test_df)\n",
    "    \n",
    "    # Train Wav2Vec2 model\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAINING OPTIMIZED WAV2VEC2 MODEL\")\n",
    "    print(\"=\"*50)\n",
    "    model, history = train_wav2vec_optimized(train_data, val_data, test_df, emotion_mapping)\n",
    "    result_file = 'wav2vec_optimized_submission.csv'\n",
    "    history_plot = 'wav2vec_optimized_history.png'\n",
    "    model_name = 'Optimized Wav2Vec2'\n",
    "    \n",
    "    # Get best accuracy\n",
    "    best_acc = max(history['val_acc'])\n",
    "    print(f\"\\nBest {model_name} Accuracy: {best_acc:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Validation')\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(history_plot)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Speech Emotion Recognition with {model_name} completed successfully!\")\n",
    "    print(f\"Results saved to {result_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e47abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with better error handling\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to saved model\n",
    "    model_path = 'checkpoints/best_wav2vec.pth'\n",
    "    \n",
    "    try:\n",
    "        # Import glob for file pattern matching\n",
    "        from glob import glob\n",
    "        \n",
    "        # Load the model\n",
    "        model, device = load_model(model_path)\n",
    "        \n",
    "        # ===== LOAD 5 TRAINING FILES =====\n",
    "        train_files = []\n",
    "        try:\n",
    "            train_df = pd.read_csv('train.csv')\n",
    "            if 'label' in train_df.columns:\n",
    "                # Try to get diverse samples (one from each emotion)\n",
    "                selected_files = []\n",
    "                unique_emotions = train_df['label'].unique()\n",
    "                for emotion in unique_emotions[:5]:  # Get up to 5 different emotions\n",
    "                    sample = train_df[train_df['label'] == emotion].sample(1)\n",
    "                    selected_files.append(f\"{sample['id'].values[0]}\")\n",
    "                \n",
    "                # If we didn't get 5 files, add more random ones\n",
    "                if len(selected_files) < 5:\n",
    "                    more_samples = train_df.sample(5 - len(selected_files))\n",
    "                    for _, row in more_samples.iterrows():\n",
    "                        selected_files.append(f\"{row['id']}\")\n",
    "                \n",
    "                # Make sure we have exactly 5 files\n",
    "                train_files = selected_files[:5]\n",
    "            else:\n",
    "                # Just get random samples if no labels\n",
    "                train_files = [f\"{id}\" for id in train_df.sample(5)['id'].values]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading train files from CSV: {e}\")\n",
    "            try:\n",
    "                train_files = glob('train/*.wav')[:5]\n",
    "            except Exception:\n",
    "                print(\"Could not find training files automatically.\")\n",
    "        \n",
    "        # ===== LOAD 5 TEST FILES =====\n",
    "        test_files = []\n",
    "        try:\n",
    "            test_df = pd.read_csv('test.csv')\n",
    "            # Get 5 random samples\n",
    "            test_files = [f\"{id}\" for id in test_df.sample(5)['id'].values]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading test files from CSV: {e}\")\n",
    "            try:\n",
    "                test_files = glob('test/*.wav')[:5]\n",
    "            except Exception:\n",
    "                print(\"Could not find test files automatically.\")\n",
    "        \n",
    "        print(f\"\\nSelected Training Files:\")\n",
    "        for file in train_files:\n",
    "            print(f\"- {file}\")\n",
    "            \n",
    "        print(f\"\\nSelected Test Files:\")\n",
    "        for file in test_files:\n",
    "            print(f\"- {file}\")\n",
    "        \n",
    "        # Verify files exist and prepend directory\n",
    "        valid_train_files = []\n",
    "        for file in train_files:\n",
    "            path = os.path.join(\"train\", file)\n",
    "            if os.path.exists(path):\n",
    "                valid_train_files.append(file)\n",
    "            else:\n",
    "                print(f\"Warning: Training file not found: {path}\")\n",
    "                \n",
    "        valid_test_files = []\n",
    "        for file in test_files:\n",
    "            path = os.path.join(\"test\", file)\n",
    "            if os.path.exists(path):\n",
    "                valid_test_files.append(file)\n",
    "            else:\n",
    "                print(f\"Warning: Test file not found: {path}\")\n",
    "        \n",
    "        if not valid_train_files and not valid_test_files:\n",
    "            raise FileNotFoundError(\"No valid audio files found to process\")\n",
    "            \n",
    "        # ===== INFERENCE ON TRAIN FILES =====\n",
    "        train_emotions = []\n",
    "        train_probs = []\n",
    "        if valid_train_files:\n",
    "            print(\"\\nProcessing training files...\")\n",
    "            train_emotions, train_probs = infer_emotion(model, valid_train_files, \"train\")\n",
    "            \n",
    "            print(\"\\nTraining File Inference Results:\")\n",
    "            for audio_file, emotion, prob in zip(valid_train_files, train_emotions, train_probs):\n",
    "                print(f\"File: {audio_file} -> Emotion: {emotion} (Confidence: {prob:.2f})\")\n",
    "        \n",
    "        # ===== INFERENCE ON TEST FILES =====\n",
    "        test_emotions = []\n",
    "        test_probs = []\n",
    "        if valid_test_files:\n",
    "            print(\"\\nProcessing test files...\")\n",
    "            test_emotions, test_probs = infer_emotion(model, valid_test_files, \"test\")\n",
    "            \n",
    "            print(\"\\nTest File Inference Results:\")\n",
    "            for audio_file, emotion, prob in zip(valid_test_files, test_emotions, test_probs):\n",
    "                print(f\"File: {audio_file} -> Emotion: {emotion} (Confidence: {prob:.2f})\")\n",
    "        \n",
    "        # ===== VISUALIZE WAVEFORMS =====\n",
    "        # Create a figure with enough subplots for all files\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot training files first\n",
    "        for i, audio_file in enumerate(valid_train_files):\n",
    "            try:\n",
    "                audio_path = os.path.join(\"train\", audio_file)\n",
    "                waveform, sample_rate = torchaudio.load(audio_path)\n",
    "                plt.subplot(len(valid_train_files) + len(valid_test_files), 1, i+1)\n",
    "                plt.plot(waveform[0].numpy())\n",
    "                plt.title(f\"TRAIN: {audio_file} - {train_emotions[i]} (Conf: {train_probs[i]:.2f})\")\n",
    "                plt.ylim([-1, 1])  # Standardize y-axis\n",
    "            except Exception as e:\n",
    "                plt.subplot(len(valid_train_files) + len(valid_test_files), 1, i+1)\n",
    "                plt.text(0.5, 0.5, f\"Error loading audio: {str(e)}\", \n",
    "                        horizontalalignment='center', verticalalignment='center')\n",
    "                plt.axis('off')\n",
    "        \n",
    "        # Then plot test files\n",
    "        offset = len(valid_train_files)\n",
    "        for i, audio_file in enumerate(valid_test_files):\n",
    "            try:\n",
    "                audio_path = os.path.join(\"test\", audio_file)\n",
    "                waveform, sample_rate = torchaudio.load(audio_path)\n",
    "                plt.subplot(len(valid_train_files) + len(valid_test_files), 1, offset+i+1)\n",
    "                plt.plot(waveform[0].numpy())\n",
    "                plt.title(f\"TEST: {audio_file} - {test_emotions[i]} (Conf: {test_probs[i]:.2f})\")\n",
    "                plt.ylim([-1, 1])  # Standardize y-axis\n",
    "            except Exception as e:\n",
    "                plt.subplot(len(valid_train_files) + len(valid_test_files), 1, offset+i+1)\n",
    "                plt.text(0.5, 0.5, f\"Error loading audio: {str(e)}\", \n",
    "                        horizontalalignment='center', verticalalignment='center')\n",
    "                plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
